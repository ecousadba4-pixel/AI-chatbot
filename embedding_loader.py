"""Utilities for loading sentence-transformer embedding models."""
from __future__ import annotations

import os
import shutil
from pathlib import Path
from typing import Iterable, Optional, Union

from sentence_transformers import SentenceTransformer

PathLike = Union[str, os.PathLike[str]]


def _expand_candidate_paths(
    candidate_paths: Optional[Iterable[Optional[PathLike]]],
) -> list[str]:
    """Return a filtered list of candidate paths that exist on disk."""
    if not candidate_paths:
        return []

    paths: list[str] = []
    for path in candidate_paths:
        if not path:
            continue
        expanded = os.path.expanduser(os.path.expandvars(os.fspath(path)))
        if os.path.exists(expanded):
            paths.append(expanded)
    return paths


def _default_model_search_paths(model_name: str) -> list[str]:
    """Return default directories to probe for a bundled embedding model."""

    model_relative = Path(*model_name.split("/"))
    project_root = Path(__file__).resolve().parent
    search_roots: list[Path] = []

    for env_var in ("EMBEDDING_MODEL_DIR", "APP_DATA_DIR", "DATA_DIR"):
        env_path = os.getenv(env_var)
        if env_path:
            search_roots.append(Path(env_path))

    search_roots.extend(
        [
            project_root / "Data",
            project_root / "data",
            project_root.parent / "Data",
            project_root.parent / "data",
            Path("/app/Data"),
            Path("/app/data"),
            Path("/data"),
        ]
    )

    candidates: list[str] = []
    seen: set[str] = set()
    for root in search_roots:
        candidate = root / model_relative
        candidate_str = os.fspath(candidate)
        if candidate_str not in seen:
            seen.add(candidate_str)
            candidates.append(candidate_str)
    return candidates


def _maybe_reassemble_shards(model_dir: Path) -> None:
    """–°–æ–±—Ä–∞—Ç—å —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏ —Ä–∞–∑–±–∏—Ç—ã –Ω–∞ —á–∞—Å—Ç–∏.

    –ù–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö –∑–∞–≥—Ä—É–∑–∫–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤–µ—Å–æ–≤ –±–æ–ª—å—à–µ 200 –ú–ë
    –∑–∞–ø—Ä–µ—â–µ–Ω–∞, –ø–æ—ç—Ç–æ–º—É –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –Ω–∞–±–æ—Ä–æ–º ``*.partXX`` —Ñ–∞–π–ª–æ–≤.
    ``SentenceTransformer`` –æ–∂–∏–¥–∞–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–µ —Ñ–∞–π–ª—ã ``model.safetensors`` –∏
    ``pytorch_model.bin`` —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏, –ø–æ—ç—Ç–æ–º—É –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π –º—ã
    —Å–æ–±–∏—Ä–∞–µ–º –∏—Ö –∑–∞–Ω–æ–≤–æ.
    """

    if not model_dir.is_dir():
        return

    shard_specs = {
        "model.safetensors": "model.safetensors.part*",
        "pytorch_model.bin": "pytorch_model.bin.part*",
    }

    for target_name, glob_pattern in shard_specs.items():
        target_path = model_dir / target_name
        if target_path.exists():
            continue

        parts = sorted(model_dir.glob(glob_pattern))
        if not parts:
            continue

        print(
            "üß© –ù–∞–π–¥–µ–Ω—ã —á–∞—Å—Ç–∏ —Ñ–∞–π–ª–∞",
            target_name,
            f"–≤ –∫–∞—Ç–∞–ª–æ–≥–µ {model_dir} ‚Äî –æ–±—ä–µ–¥–∏–Ω—è–µ–º ({len(parts)} —à—Ç.)",
        )
        temp_path = target_path.parent / f"{target_path.name}.tmp"
        with open(temp_path, "wb") as target_file:
            for part in parts:
                with open(part, "rb") as part_file:
                    shutil.copyfileobj(part_file, target_file)

        os.replace(temp_path, target_path)
        print(f"‚úÖ –§–∞–π–ª {target_path.name} –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–∑ —á–∞—Å—Ç–µ–π")


def resolve_embedding_model(
    model_name: str,
    candidate_paths: Optional[Iterable[Optional[str]]] = None,
    *,
    allow_download: bool = True,
) -> SentenceTransformer:
    """Load a sentence-transformer model.

    The function first tries to load the model from the provided ``candidate_paths``.
    If none of the paths contain the model, it falls back to downloading the model by
    name using :class:`SentenceTransformer`.
    """

    search_candidates: list[Optional[PathLike]] = []
    if candidate_paths:
        search_candidates.extend(candidate_paths)
    search_candidates.extend(_default_model_search_paths(model_name))

    expanded_paths = _expand_candidate_paths(search_candidates)

    summary: list[str] = []

    if expanded_paths:
        print("üîé –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø—É—Ç–∏ –º–æ–¥–µ–ª–∏:")
    for path in expanded_paths:
        print(f"  ‚Ä¢ üîç {path}")
        try:
            print(f"    ‚Ü™Ô∏è –ü—ã—Ç–∞–µ–º—Å—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —à–∞—Ä–¥—ã –º–æ–¥–µ–ª–∏ –≤ {path}")
            _maybe_reassemble_shards(Path(path))
        except Exception as shard_error:
            print(
                "    ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —à–∞—Ä–¥—ã:",
                shard_error,
            )
            summary.append(f"‚ö†Ô∏è {path} ‚Äî –æ—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —à–∞—Ä–¥–æ–≤: {shard_error}")
        else:
            summary.append(f"‚ÑπÔ∏è {path} ‚Äî –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —à–∞—Ä–¥–æ–≤ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –∏–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

        try:
            model = SentenceTransformer(path)
            setattr(model, "_resolved_from", path)
            print(f"üß† –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ {path}")
            summary.append(f"‚úÖ {path} ‚Äî –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            print("üìã –°–≤–æ–¥–∫–∞ –ø–æ –ª–æ–∫–∞–ª—å–Ω—ã–º –ø—É—Ç—è–º:")
            for item in summary:
                print(f"  ‚Ä¢ {item}")
            return model
        except Exception as load_error:
            print(f"    ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∏–∑ {path}: {load_error}")
            summary.append(f"‚ùå {path} ‚Äî –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {load_error}")
            # Path exists but does not contain a valid model.
            continue

    if expanded_paths:
        print("üìã –°–≤–æ–¥–∫–∞ –ø–æ –ª–æ–∫–∞–ª—å–Ω—ã–º –ø—É—Ç—è–º:")
        for item in summary:
            print(f"  ‚Ä¢ {item}")
    else:
        print("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –ø—É—Å—Ç ‚Äî –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –∑–∞–≥—Ä—É–∑–∫–µ –ø–æ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏.")

    if not allow_download:
        searched = "\n".join(f" - {p}" for p in expanded_paths)
        raise FileNotFoundError(
            "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤. "
            "–£–∫–∞–∂–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è EMBEDDING_MODEL_PATH (–∏–ª–∏ EMBEDDING_MODEL_DIR/APP_DATA_DIR/DATA_DIR) "
            "–∏ —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è –≤ –æ–¥–Ω–æ–º –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö –ø—É—Ç–µ–π:\n"
            f"{searched if searched else ' - (—Å–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –ø—É—Å—Ç)'}"
        )

    print(
        "üåê –õ–æ–∫–∞–ª—å–Ω—ã–µ –ø—É—Ç–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ø–æ –∏–º–µ–Ω–∏:",
        model_name,
    )
    model = SentenceTransformer(model_name)
    setattr(model, "_resolved_from", model_name)
    print(f"üß† –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ø–æ –∏–º–µ–Ω–∏ {model_name}")
    return model


